{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbaVUT78Gutx"
      },
      "outputs": [],
      "source": [
        "#Q1 What is a parameter?\n",
        "\n",
        "\"\"\"A parameter is a variable used to pass information into a function, method, or procedure.\n",
        "\n",
        "Here’s a breakdown depending on context:\n",
        "\n",
        "In Programming:\n",
        "A parameter is a placeholder that a function uses to receive values. These values (called arguments when passed in) are supplied when the function is called.\n",
        "\n",
        "\n",
        "def greet(name):  # \"name\" is the parameter\n",
        "    print(\"Hello, \" + name)\n",
        "\n",
        "greet(\"Alice\")    # \"Alice\" is the argument\n",
        "\n",
        "In Math:\n",
        "A parameter is a constant that defines or influences the behavior of a function or equation.\n",
        "\n",
        "Example:\n",
        "In the equation of a straight line:\n",
        "y = mx + b,\n",
        "\n",
        "m and b are parameters.\n",
        "\n",
        "x is the variable.\n",
        "\n",
        "In Statistics/Science:\n",
        "A parameter is a measurable factor that defines a system or sets conditions of an experiment or model.\n",
        "\n",
        "Example:\n",
        "\n",
        "The average height of a population is a parameter.\n",
        "\n",
        "You might estimate it using a statistic from a sample.\n",
        "\n",
        "\"\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.What is correlation? What does negative correlation mean?\n",
        "\n",
        "\"\"\"\"What is Correlation?\n",
        "Correlation is a statistical measure that describes how two variables move in relation to each other. It tells you if there is a relationship and how strong that relationship is.\n",
        "\n",
        "It’s often measured using the correlation coefficient (r), which ranges from -1 to 1:\n",
        "\n",
        "+1 = perfect positive correlation\n",
        "\n",
        "0 = no correlation\n",
        "\n",
        "-1 = perfect negative correlation\n",
        "\n",
        "Types of Correlation:\n",
        "Positive Correlation\n",
        "As one variable increases, the other also increases.\n",
        "Example:\n",
        "\n",
        "Hours studied  ↔ Exam score\n",
        "More studying usually means better scores.\n",
        "\n",
        " Negative Correlation\n",
        "As one variable increases, the other decreases.\n",
        "Example:\n",
        "\n",
        "Amount of exercise  ↔ Weight\n",
        "More exercise tends to lower weight.\n",
        "\n",
        "No Correlation\n",
        "No clear pattern between the variables.\n",
        "Example:\n",
        "\n",
        "Shoe size  ↔ Test scores\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "Ultpxae4HttA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "\"\"\"\"hat is Machine Learning (ML)?\n",
        "Machine Learning is a branch of Artificial Intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed for every task.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "It’s like teaching a computer how to spot patterns and make smart choices—by giving it examples instead of step-by-step instructions.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "Here are the key building blocks that make ML work:\n",
        "\n",
        "1. Data\n",
        "The foundation of ML.\n",
        "\n",
        "Examples: images, numbers, text, clicks, sales records, etc.\n",
        "\n",
        "The better and cleaner your data, the better your model can learn.\n",
        "\n",
        "2. Features\n",
        "Individual measurable properties or characteristics of the data.\n",
        "\n",
        "Example: For a house price prediction model, features could be:\n",
        "\n",
        "Square footage\n",
        "\n",
        "Number of bedrooms\n",
        "\n",
        "Location\n",
        "\n",
        "3. Model\n",
        "The mathematical representation or system that learns from data.\n",
        "\n",
        "It maps inputs (features) to outputs (predictions).\n",
        "\n",
        "4. Algorithm\n",
        "The learning method used to train the model.\n",
        "\n",
        "Examples: Linear Regression, Decision Trees, Neural Networks, etc.\n",
        "\n",
        "It defines how the model will learn patterns from the data.\n",
        "\n",
        "5. Training\n",
        "Feeding data into the model so it can learn.\n",
        "\n",
        "The goal is to minimize error between predicted and actual outputs.\n",
        "\n",
        "6. Testing / Evaluation\n",
        "After training, we test the model with new data to check how well it performs.\n",
        "\n",
        "Common metrics: Accuracy, Precision, Recall, F1 Score, etc.\n",
        "\n",
        "7. Prediction / Inference\n",
        "Once trained, the model can predict outcomes for new, unseen data.\n",
        "\n",
        "Example: Predicting if an email is spam or not based on its content.\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "LtmZXQnJIIjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4.How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "\"\"\"\"The loss is a number that tells you how far off your model's predictions are from the actual targets. It's calculated using a loss function, like Mean Squared Error for regression or Cross-Entropy for classification.\n",
        "\n",
        "Lower loss = better performance (generally).\n",
        "\n",
        "It’s used by the optimizer (like Adam or SGD) to adjust the model's weights during training.\n",
        "\n",
        "How It Helps Determine Model Quality\n",
        "Training Progress:\n",
        "\n",
        "If the loss is decreasing steadily, it means the model is learning.\n",
        "\n",
        "If the loss stagnates or increases, something might be wrong (e.g., bad learning rate, overfitting, poor model architecture).\n",
        "\n",
        "Overfitting Detection:\n",
        "\n",
        "Compare training loss vs. validation loss:\n",
        "\n",
        "If training loss is low but validation loss is high: your model is likely overfitting.\n",
        "\n",
        "If both are high: underfitting or not enough training.\n",
        "\n",
        "If both are low: you're likely in a good spot.\n",
        "\n",
        "Model Comparison:\n",
        "\n",
        "Use the loss to compare different models or training runs. The model with the lowest validation loss is usually the best one.\n",
        "\n",
        "Not the Whole Story:\n",
        "\n",
        "A good loss doesn’t always mean good real-world performance. For classification tasks, accuracy, precision, recall, etc., are also important.\n",
        "\n",
        "But loss gives a more granular view because it reflects how confident or close the predictions are—not just whether they’re right or wrong.\n",
        "\n",
        "Example\n",
        "Let’s say you’re training a binary classifier:\n",
        "\n",
        "Epoch\tTraining Loss\tValidation Loss\tAccuracy\n",
        "1\t0.68\t0.69\t60%\n",
        "5\t0.45\t0.50\t75%\n",
        "10\t0.30\t0.55\t80%\n",
        "Here, even though training loss is going down, validation loss starts increasing after epoch 5 → sign of overfitting.\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "eWQZEC_CIbrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5.What are continuous and categorical variables?\n",
        "\n",
        "\"\"\"\"Categorical Variables\n",
        "These are variables that represent categories or groups. They have a finite set of possible values, and those values don’t have mathematical meaning (usually).\n",
        "\n",
        "Examples:\n",
        "Gender: Male, Female, Non-binary\n",
        "\n",
        "Color: Red, Blue, Green\n",
        "\n",
        "Education Level: High School, Bachelor’s, Master’s, PhD\n",
        "\n",
        "Yes/No: Like a binary categorical variable\n",
        "\n",
        "Key Points:\n",
        "Can be nominal (no natural order): e.g., color, gender\n",
        "\n",
        "Or ordinal (have an order): e.g., education level, shirt sizes (S < M < L)\n",
        "\n",
        "Usually encoded with One-Hot Encoding or Label Encoding in ML models\n",
        "\n",
        "Continuous Variables\n",
        "These are numeric variables that can take an infinite number of values within a range. Think of measurements, counts, or quantities that can be broken down infinitely.\n",
        "\n",
        "Examples:\n",
        "Age (e.g., 21.5 years)\n",
        "\n",
        "Temperature (e.g., 98.6°F)\n",
        "\n",
        "Height, Weight, Income, Time, Distance\n",
        "\n",
        "Key Points:\n",
        "Values are measurable and often have decimals\n",
        "\n",
        "You can compute mean, median, standard deviation, etc.\n",
        "\n",
        "Often used directly in models (sometimes need normalization/scaling)\n",
        "\n",
        "TL;DR\n",
        "Type\tExamples\tCan Be Ordered?\tUsed In\n",
        "Categorical\tGender, Color, City\tSometimes (ordinal)\tClassification, grouping\n",
        "Continuous\tHeight, Salary, Temperature\tYes\tRegression, stats, ML features\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "oIMB5z7ASgfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "\"\"\"\"Handling categorical variables in machine learning is crucial because most ML models can only handle numerical input. Categorical variables represent data types that contain label values rather than numeric values (e.g., \"red\", \"blue\", \"green\" or \"low\", \"medium\", \"high\").\n",
        "\n",
        "Here are common techniques to handle them:\n",
        "\n",
        "1. Label Encoding\n",
        "Assigns each unique category an integer value.\n",
        "\n",
        "Example: {\"red\": 0, \"blue\": 1, \"green\": 2}\n",
        "\n",
        "Useful for tree-based models (e.g., Decision Trees, Random Forest).\n",
        "\n",
        "Not ideal for linear models (they might assume ordinal relationships where there are none).\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates binary columns for each category.\n",
        "\n",
        "Example: \"red\", \"blue\", \"green\" → [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "Widely used for nominal (non-ordered) categories.\n",
        "\n",
        "Can lead to high dimensionality if the variable has many unique categories (\"curse of dimensionality\").\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Similar to label encoding but used for categories with a meaningful order.\n",
        "\n",
        "Example: {\"low\": 1, \"medium\": 2, \"high\": 3}\n",
        "\n",
        "Suitable when the categories have a clear ranking.\n",
        "\n",
        "Should not be used for nominal categories.\n",
        "\n",
        "4. Target Encoding (Mean Encoding)\n",
        "Replace a category with the mean of the target variable for that category.\n",
        "\n",
        "Example: If the average house price for \"city A\" is $300K and \"city B\" is $250K, you encode \"city A\" as 300 and \"city B\" as 250.\n",
        "\n",
        "Can capture useful target-related signals.\n",
        "\n",
        "Prone to overfitting (should use cross-validation or smoothing).\n",
        "\n",
        "5. Binary Encoding\n",
        "Combines the benefits of one-hot and label encoding.\n",
        "\n",
        "Encodes categories into binary numbers, then splits digits into separate columns.\n",
        "\n",
        "Good trade-off between dimensionality and information.\n",
        "\n",
        "Less interpretable.\n",
        "\n",
        "6. Frequency / Count Encoding\n",
        "Replace each category with its frequency/count in the dataset.\n",
        "\n",
        "Simple, can be useful with tree models.\n",
        "\n",
        "Loses uniqueness of the original categories.\n",
        "\n",
        "7. Embeddings (for Deep Learning)\n",
        "Learn dense vector representations of categories during model training.\n",
        "\n",
        "Common in NLP and recommendation systems.\n",
        "\n",
        "Powerful with large datasets and complex relationships.\n",
        "\n",
        "Requires more complex setup (neural networks).\n",
        "\n",
        "Choosing the Right Method:\n",
        "For tree-based models (Random Forest, XGBoost): label, frequency, or target encoding often work well.\n",
        "\n",
        "For linear models (Logistic Regression, SVM): one-hot or ordinal encoding (if order exists).\n",
        "\n",
        "For deep learning: embedding layers shine.\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "h3ppPiaBS2md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7.What do you mean by training and testing a dataset?\n",
        "\n",
        "\"\"\"\"When we train a machine learning model, we give it a portion of the data (called the training set) so it can learn the patterns, relationships, and features that link input variables (features) to the output (target).\n",
        "\n",
        "Think of it like a student studying from a textbook.\n",
        "\n",
        "The model adjusts its internal parameters (e.g., weights in linear regression or splits in a decision tree) to minimize errors on this data.\n",
        "\n",
        "What is Testing a Dataset?\n",
        "Once the model is trained, we need to check how well it performs on new data it hasn’t seen before. That’s where the test set comes in.\n",
        "\n",
        "It’s like giving the student an exam on new questions (but from the same subject).\n",
        "\n",
        "If the model performs well here, it means it’s likely to perform well on real-world data.\n",
        "\n",
        "Why Do We Split the Data?\n",
        "To evaluate the model’s generalization—its ability to work on unseen data rather than just memorizing the training data.\n",
        "\n",
        "A typical split might be:\n",
        "\n",
        "70% training\n",
        "\n",
        "30% testing or\n",
        "\n",
        "80% training\n",
        "\n",
        "20% testing\n",
        "\n",
        "Sometimes we also have a validation set in addition to training and test:\n",
        "\n",
        "Training set → to train the model.\n",
        "\n",
        "Validation set → to tune hyperparameters and avoid overfitting.\n",
        "\n",
        "Test set → final evaluation.\n",
        "\n",
        "Example:\n",
        "Imagine you're predicting house prices.\n",
        "\n",
        "Features\tPrice ($)\n",
        "Size, Location, Age\t300,000\n",
        "Size, Location, Age\t250,000\n",
        "...\t...\n",
        "You train your model on 80% of the rows.\n",
        "\n",
        "Then you test it on the remaining 20% to see how well it predicts prices for houses it hasn't seen before.\n",
        "\n",
        "\"\"\"\"\""
      ],
      "metadata": {
        "id": "CQ9JhBvoV2nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8.What is sklearn.preprocessing?\n",
        "\n",
        "\"\"\"\"sklearn.preprocessing is a module in Scikit-learn (a popular machine learning library in Python) that provides tools to prepare your data before feeding it into a machine learning model.\n",
        "\n",
        "Think of it like the kitchen prep before cooking—you get the ingredients (data) cleaned, chopped, and ready to use!\n",
        "\n",
        "Why is Preprocessing Important?\n",
        "Most ML algorithms expect data to be in a certain format (usually numerical, scaled, no missing values, etc.). Preprocessing helps with:\n",
        "\n",
        "Converting categorical data into numbers\n",
        "\n",
        "Scaling/normalizing features\n",
        "\n",
        "Handling missing values\n",
        "\n",
        "Generating polynomial features\n",
        "\n",
        "Encoding labels and targets\n",
        "\n",
        "Common Tools in sklearn.preprocessing:\n",
        "1. StandardScaler\n",
        "Scales features to have zero mean and unit variance.\n",
        "\n",
        "Often used for algorithms like SVM, KNN, Logistic Regression.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "2. MinMaxScaler\n",
        "Scales data to a specific range, typically [0, 1].\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "3. LabelEncoder\n",
        "Converts categorical labels (targets) into integers.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "4. OneHotEncoder\n",
        "Converts categorical features into a one-hot numeric array.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(X_categorical)\n",
        "5. PolynomialFeatures\n",
        "Generates new features by combining existing features to a certain polynomial degree.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "6. Binarizer\n",
        "Converts numeric features into 0s and 1s based on a threshold.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold=0.5)\n",
        "X_bin = binarizer.fit_transform(X)\n",
        "7. Normalizer\n",
        "Scales input vectors individually to unit norm (for text classification, cosine similarity, etc.).\n",
        "\n",
        "Bonus: Pipelines\n",
        "You often use these preprocessing tools in a pipeline with your model:\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "oBZSDoF2XPSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9.What is a Test set?\n",
        "\n",
        "\"\"\"\" A test set is a portion of your dataset that you set aside to evaluate the final performance of your trained machine learning model.\n",
        "\n",
        "Think of it as the \"final exam\" for your model.\n",
        "\n",
        "Purpose of a Test Set:\n",
        "To check how well your model performs on unseen data.\n",
        "\n",
        "Helps detect overfitting (when the model does great on training data but poorly on new data).\n",
        "\n",
        "Gives you an honest estimate of how your model will perform in the real world.\n",
        "\n",
        "How It Works:\n",
        "When you build a machine learning model, your full dataset is usually split into:\n",
        "\n",
        "Dataset Part\tPurpose\tTypical Size\n",
        "Training set\tLearn patterns from data\t70–80% of data\n",
        "Test set\tEvaluate final model performance\t20–30% of data\n",
        "Example:\n",
        "Let's say you have 1000 data points.\n",
        "\n",
        "You train your model on 800 (training set).\n",
        "\n",
        "You test it on the remaining 200 (test set).\n",
        "\n",
        "The model has never seen those 200 examples during training.\n",
        "\n",
        "This way, you can measure metrics like accuracy, precision, recall, RMSE, etc., on the test set to see how the model truly performs.\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "d8mCmvByXqa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10.How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "\"\"\"\"1. How to Split Data for Model Fitting (Training and Testing) in Python\n",
        "We usually use train_test_split from sklearn.model_selection. Here's how it works:\n",
        "\n",
        "Basic Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume X = features, y = labels/target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,     # 20% for testing, 80% for training\n",
        "    random_state=42    # ensures the same split every time (reproducibility))\n",
        "What it does:\n",
        "X_train, y_train: used to train your model\n",
        "\n",
        "X_test, y_test: used to evaluate how your model performs on new/unseen data\n",
        "\n",
        "You can also add stratify=y to ensure the class distribution is the same in both sets (useful in classification problems).\n",
        "\n",
        "2. How Do You Approach a Machine Learning Problem?\n",
        "Here's a standard and practical ML workflow:\n",
        "\n",
        "Step-by-Step ML Pipeline:\n",
        "1. Understand the Problem\n",
        "What are we predicting?\n",
        "\n",
        "Is it classification or regression?\n",
        "\n",
        "2. Collect and Explore the Data\n",
        "Load your data (CSV, DB, API, etc.)\n",
        "\n",
        "Check shape, missing values, data types\n",
        "\n",
        "Use tools like pandas, matplotlib, seaborn\n",
        "\n",
        "3. Clean and Preprocess\n",
        "Handle missing values\n",
        "\n",
        "Encode categorical variables (LabelEncoder, OneHotEncoder)\n",
        "\n",
        "Scale features (StandardScaler, MinMaxScaler)\n",
        "\n",
        "4. Split the Data\n",
        "Use train_test_split() as shown above\n",
        "\n",
        "Optionally create a validation set or use cross-validation\n",
        "\n",
        "5. Choose and Train a Model\n",
        "Try models like:\n",
        "\n",
        "Linear Regression, Decision Tree, Random Forest, XGBoost (for structured data)\n",
        "\n",
        "Logistic Regression, KNN, SVM, etc.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "6. Evaluate the Model\n",
        "Use metrics like accuracy, F1-score, confusion matrix, RMSE, etc.\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "7. Tune the Model\n",
        "Use GridSearchCV or RandomizedSearchCV\n",
        "\n",
        "Adjust hyperparameters (like depth of a tree, learning rate, etc.)\n",
        "\n",
        "8. Deploy or Communicate Results\n",
        "Save model with joblib or pickle\n",
        "\n",
        "Build a dashboard (Streamlit, Flask, etc.)\n",
        "\n",
        "Present insights from the model\n",
        "\"\"\"\"\n"
      ],
      "metadata": {
        "id": "rO7FCRThX_MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11.Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "\"\"\"\"EDA = Exploring and understanding your data before modeling it.\n",
        "\n",
        "You use visualizations, summary statistics, and data checks to:\n",
        "\n",
        "Understand the structure and quality of the data\n",
        "\n",
        "Detect patterns, relationships, and outliers\n",
        "\n",
        "Decide how to clean, transform, or feature engineer your data\n",
        "\n",
        "Why Is EDA So Important Before Modeling?\n",
        "1. Understand Your Data\n",
        "What's in the dataset? What are the features and target?\n",
        "\n",
        "Are variables categorical, numerical, or datetime?\n",
        "\n",
        "How are they distributed?\n",
        "\n",
        "2. Identify Data Quality Issues\n",
        "Missing values? Duplicates?\n",
        "\n",
        "Inconsistent or incorrect values?\n",
        "\n",
        "Noise or irrelevant features?\n",
        "\n",
        "🛠 Example: If 30% of values in a column are missing, you need to handle that before modeling.\n",
        "\n",
        "3. Spot Outliers and Skewness\n",
        "Outliers can distort models (especially linear models).\n",
        "\n",
        "Skewed data may need transformation (e.g., log-scaling).\n",
        "\n",
        "4. Feature Relationships\n",
        "Are any features strongly correlated?\n",
        "\n",
        "Is the target variable related to any specific features?\n",
        "\n",
        "Visuals like scatter plots, pairplots, and correlation heatmaps are super helpful here.\n",
        "\n",
        "5. Guide Feature Engineering\n",
        "Maybe two features should be combined (e.g., date of birth → age).\n",
        "\n",
        "Maybe you need to encode text data or create interaction terms.\n",
        "\n",
        "6. Choose the Right Model or Preprocessing\n",
        "If your features have vastly different scales → you might need scaling.\n",
        "\n",
        "If your target is imbalanced (e.g., fraud detection), you’ll need special handling like SMOTE or class weights.\n",
        "\n",
        "EDA Tools and Techniques:\n",
        "df.info(), df.describe(), df.isnull().sum() (basic data checks)\n",
        "\n",
        "Histograms, boxplots, scatter plots, pairplots (for distributions and relationships)\n",
        "\n",
        "Correlation heatmaps (seaborn.heatmap)\n",
        "\n",
        "Groupby and pivot tables (for aggregating by categories)\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "Mkq-55vNYvXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12.What is correlation?\n",
        "\n",
        "\"\"\"\"What is Correlation?\n",
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "In simple terms:\n",
        "If one variable changes, does the other tend to change too? And in which direction?\n",
        "\n",
        "Correlation Values\n",
        "Correlation is usually measured using the Pearson correlation coefficient (r), which ranges from -1 to +1:\n",
        "\n",
        "Value of r\tInterpretation\n",
        "+1\tPerfect positive correlation\n",
        "0\tNo correlation\n",
        "-1\tPerfect negative correlation\n",
        "Examples:\n",
        "Positive correlation (r > 0):\n",
        "As X increases, Y also increases.\n",
        "Example: Height vs. Weight.\n",
        "\n",
        "Negative correlation (r < 0):\n",
        "As X increases, Y decreases.\n",
        "Example: Number of hours watching Netflix vs. Exam score\n",
        "\n",
        "No correlation (r ≈ 0):\n",
        "The variables don’t move in sync.\n",
        "\n",
        "Why Is Correlation Important in ML?\n",
        "Helps in feature selection: You may drop one of two highly correlated features (multicollinearity).\n",
        "\n",
        "Understand how variables are related to the target.\n",
        "\n",
        "Can detect data leakage: If a feature is too correlated with the target, it might be leaking info from the future.\n",
        "\n",
        "How to Calculate in Python:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Visualize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "nWzj6iLIZLPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13.What does negative correlation mean?\n",
        "\n",
        "\"\"\"\"What Is Negative Correlation?\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "It’s like a see-saw: when one side goes up, the other goes down.\n",
        "\n",
        "The correlation coefficient (r) will be less than 0, ranging from 0 to -1.\n",
        "\n",
        " Interpreting Values:\n",
        "Correlation (r)\tStrength\n",
        "-1.0\tPerfect negative correlation\n",
        "-0.7 to -1.0\tStrong negative correlation\n",
        "-0.3 to -0.7\tModerate negative correlation\n",
        "0 to -0.3\tWeak or no negative correlation\n",
        " Example Scenarios:\n",
        "Hours spent partying vs. Exam scores:\n",
        "More party time  → Lower grades\n",
        "\n",
        "Price of a product vs. Demand:\n",
        "Higher prices  → Lower demand\n",
        "\n",
        "Altitude vs. Temperature:\n",
        "Higher up a mountain  → Colder temps\n",
        "\n",
        "Visual Example:\n",
        "Imagine plotting two variables on a scatter plot. In negative correlation:\n",
        "\n",
        "Dots slope downward from left to right.\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "px5p7qKYZ-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14.How can you find correlation between variables in Python?\n",
        "\n",
        "\"\"\"\"Step-by-Step: How to Find Correlation in Python\n",
        "1. Import Libraries and Load Data\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset (example using seaborn's built-in dataset)\n",
        "df = sns.load_dataset(\"iris\")  # or pd.read_csv(\"yourfile.csv\")\n",
        "2. Use .corr() to Compute the Correlation Matrix\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        "This gives you a matrix showing the Pearson correlation coefficient between every pair of numerical columns.\n",
        "\n",
        "Only works on numerical columns by default.\n",
        "\n",
        "3. Visualize It with a Heatmap (optional but highly recommended)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "This color-coded heatmap helps you easily spot strong positive (closer to +1) and strong negative (closer to -1) correlations.\n",
        "\n",
        "Optional: Choose Other Correlation Methods\n",
        ".corr() supports different methods:\n",
        "\n",
        "df.corr(method='pearson')   # default, linear relationship\n",
        "df.corr(method='kendall')   # rank-based, more robust for small datasets\n",
        "df.corr(method='spearman')  # monotonic relationships (not just linear)\n",
        "Example Output (for Iris dataset):\n",
        "sepal_length\tsepal_width\tpetal_length\tpetal_width\n",
        "sepal_length\t1.00\t-0.12\t0.87\t0.82\n",
        "sepal_width\t-0.12\t1.00\t-0.43\t-0.37\n",
        "petal_length\t0.87\t-0.43\t1.00\t0.96\n",
        "petal_width\t0.82\t-0.37\t0.96\t1.00\n",
        "You can see strong positive correlation between petal_length and petal_width, and a negative correlation between sepal_width and others.\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "EkaVjMrqaTDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15.What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "\"\"\"\"What is Causation?\n",
        "Causation means that one event directly causes another to happen.\n",
        "\n",
        "A → B\n",
        "\n",
        "It’s a cause-and-effect relationship.\n",
        "\n",
        "Example of Causation:\n",
        "Lighting a match causes fire.\n",
        "\n",
        "Pushing a ball causes it to roll.\n",
        "\n",
        "There's a direct link between the action and the outcome.\n",
        "\n",
        "What is Correlation?\n",
        "Correlation means that two variables are related or move together, but one does not necessarily cause the other.\n",
        "\n",
        "A ↔ B (they’re related, but not necessarily one causing the other)\n",
        "\n",
        "Example of Correlation:\n",
        "Ice cream sales and drowning incidents both go up in summer.\n",
        "\n",
        " ↑ → each time ↑ → More swimming → More accidents\n",
        "\n",
        "They’re correlated (both go up), but eating ice cream doesn’t cause drowning. The real cause is summer (a hidden third factor called a confounder).\n",
        "\n",
        "Key Differences:\n",
        "Aspect\tCorrelation\tCausation\n",
        "Direction\tVariables change together\tOne variable drives change in another\n",
        "Proof\tSuggests a relationship\tRequires experiments or strong evidence\n",
        "Example\tCoffee and productivity\tDrinking coffee improves alertness\n",
        "Type\tStatistical\tLogical or scientific\n",
        "In Machine Learning:\n",
        "We often detect correlation, but we have to be very careful not to assume causation.\n",
        "\n",
        "Causation needs controlled experiments, time-order evidence, or causal inference methods (like A/B testing, DAGs, or statistical controls).\n",
        "Quick Rule of Thumb:\n",
        "Just because two things happen together doesn’t mean one is causing the other.\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "jS5cmo5hax6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "\"\"\"\"What is an Optimizer?\n",
        "An optimizer is an algorithm that adjusts the model's parameters (like weights) to minimize the loss function and improve performance during training.\n",
        "\n",
        "In simple terms:\n",
        "It helps the model learn by improving it step by step after each prediction.\n",
        "\n",
        "Why Do We Need Optimizers?\n",
        "When we train a model, we want it to make better predictions over time. The optimizer tweaks the model’s internal parameters to reduce the error (loss) based on the feedback (gradient).\n",
        "\n",
        "Common Types of Optimizers (Especially in Deep Learning)\n",
        "Here are the most popular ones, especially in neural networks using libraries like TensorFlow or PyTorch:\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "The most basic optimizer. It updates parameters using the entire dataset each time.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "θ = θ - α * ∇L(θ)\n",
        "θ = parameters (weights)\n",
        "\n",
        "α = learning rate\n",
        "\n",
        "∇L(θ) = gradient of loss\n",
        "\n",
        "Pros:\n",
        "Simple\n",
        "\n",
        "Cons:\n",
        "Slow for large datasets\n",
        "\n",
        "Requires full dataset every update\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "Instead of the full dataset, SGD uses one random sample at a time.\n",
        "\n",
        "Pros:\n",
        "Faster updates\n",
        "\n",
        "Cons:\n",
        "More noisy, can bounce around\n",
        "\n",
        "\n",
        "from torch.optim import SGD\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "3. Mini-Batch Gradient Descent\n",
        "A middle ground: updates using a small batch of samples (e.g., 32, 64).\n",
        "\n",
        "It balances speed and stability.\n",
        "\n",
        "Most commonly used in practice.\n",
        "\n",
        "4. Momentum\n",
        "Adds a “memory” of previous updates to avoid getting stuck and speed up learning.\n",
        "\n",
        "Analogy:\n",
        "Like rolling a ball down a hill—it picks up speed over time.\n",
        "\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "5. RMSProp\n",
        "Adapts the learning rate for each parameter, using a moving average of the gradients' squared values.\n",
        "\n",
        "Great for recurrent neural networks (RNNs).\n",
        "\n",
        "\n",
        "from torch.optim import RMSprop\n",
        "optimizer = RMSprop(model.parameters(), lr=0.001)\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "Combines the ideas of Momentum and RMSProp:\n",
        "\n",
        "Keeps track of both the average gradient and the average squared gradient\n",
        "\n",
        "Works really well in most cases—default optimizer in deep learning\n",
        "\n",
        "\n",
        "from torch.optim import Adam\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "Pros:\n",
        "Fast convergence\n",
        "\n",
        "Widely used, stable\n",
        "\n",
        "Cons:\n",
        "Can overfit if not tuned properly\n",
        "\n",
        "Summary Table:\n",
        "Optimizer\tKey Feature\tBest For\n",
        "GD\tFull dataset each update\tSmall datasets\n",
        "SGD\tOne sample at a time\tOnline learning, fast training\n",
        "Mini-Batch GD\tBatches of data\tReal-world training\n",
        "Momentum\tAdds velocity\tSpeeds up SGD\n",
        "RMSProp\tAdapts learning rate per weight\tRNNs, unstable gradients\n",
        "Adam\tAdaptive + momentum\tMost deep learning tasks\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "zqlo2SL7bQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17.What is sklearn.linear_model ?\n",
        "\n",
        "\"\"\"\"What is sklearn.linear_model?\n",
        "sklearn.linear_model is a module in Scikit-learn that provides a collection of linear models for regression and classification tasks.\n",
        "\n",
        "These models assume a linear relationship between input features (X) and the output (y).\n",
        "\n",
        "What Can You Do with linear_model?\n",
        "You can use it for:\n",
        "\n",
        "Problem Type\tModels Available\n",
        "Regression\tLinearRegression, Ridge, Lasso, ElasticNet\n",
        "Classification\tLogisticRegression, SGDClassifier, Perceptron\n",
        "Commonly Used Models:\n",
        "1. Linear Regression\n",
        "Used for predicting a continuous output (e.g., house price).\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "2. Logistic Regression\n",
        "Used for binary or multi-class classification (e.g., spam or not spam).\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "3. Ridge Regression (L2 regularization)\n",
        "Helps reduce overfitting by adding a penalty for large coefficients.\n",
        "\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "4. Lasso Regression (L1 regularization)\n",
        "Performs both shrinkage and feature selection (can reduce some weights to zero).\n",
        "\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "5. ElasticNet (L1 + L2 mix)\n",
        "Combines both Lasso and Ridge—it balances between reducing model complexity and keeping important features.\n",
        "\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "When to Use sklearn.linear_model\n",
        "When you assume linearity between input features and output\n",
        "\n",
        "For interpretable models\n",
        "\n",
        "To start with a simple baseline before moving to complex models\n",
        "\n",
        "Bonus: How It Fits in the ML Pipeline\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n",
        "model.fit(X_train, y_train)\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "OYTOglTscId9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18.What does model.fit() do? What arguments must be given?\n",
        "\n",
        "\"\"\"\"What Does model.fit() Do?\n",
        "model.fit() is the method that trains your model.\n",
        "\n",
        "It tells the model to learn the patterns from the training data.\n",
        "\n",
        "What Happens Internally?\n",
        "When you call:\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "Here’s what happens behind the scenes:\n",
        "\n",
        "The model looks at X_train (input features) and y_train (labels/targets).\n",
        "\n",
        "It learns the relationship between inputs and outputs.\n",
        "\n",
        "It stores the learned parameters (like weights in linear models or splits in decision trees).\n",
        "\n",
        "Now it’s ready to make predictions with .predict().\n",
        "\n",
        "Arguments You Must Pass:\n",
        "Basic (most common):\n",
        "Argument\tDescription\tExample\n",
        "X_train\tFeature matrix (inputs)\tShape: (n_samples, n_features)\n",
        "y_train\tTarget values (outputs/labels)\tShape: (n_samples,)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "Example with LinearRegression:\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)  # X = input features, y = target\n",
        "Optional (for some models):\n",
        "Some models allow additional arguments like:\n",
        "\n",
        "sample_weight: gives different importance to different samples\n",
        "\n",
        "classes: for some classifiers\n",
        "\n",
        "eval_set: for early stopping in some gradient boosting models\n",
        "\n",
        "Example:\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train, sample_weight=weights)\n",
        "Shapes Matter\n",
        "Make sure:\n",
        "\n",
        "X_train is 2D: shape = (rows, columns)\n",
        "\n",
        "y_train is 1D: shape = (rows,) (for regression) or (rows,)/(rows, classes) for classification\n",
        "\n",
        "After .fit() What’s Next?\n",
        "Use .predict(X_test) to make predictions\n",
        "\n",
        "Use .score(X, y) or metrics like accuracy or RMSE to evaluate\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "FLfjKaTMcwzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "\"\"\"\"What Does model.predict() Do?\n",
        "model.predict() is used to make predictions on new data after your model has been trained using model.fit().\n",
        "\n",
        "It uses the patterns the model learned during training to predict target values (labels or outputs) for unseen input data.\n",
        "\n",
        "Typical Workflow:\n",
        "\n",
        "model.fit(X_train, y_train)      # Train the model\n",
        "predictions = model.predict(X_test)  # Predict on new (test) data\n",
        "Arguments Required\n",
        "Argument\tDescription\tShape\n",
        "X_test\tNew input data (features only)\tShape: (n_samples, n_features)\n",
        "Note: You only pass features, not the target/label.\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "Output:\n",
        "For regression:\n",
        "Returns an array of predicted values (floats).\n",
        "\n",
        "Example: [102.3, 98.7, 110.5]\n",
        "\n",
        "For classification:\n",
        "Returns predicted class labels.\n",
        "\n",
        "Example: ['spam', 'ham', 'ham'] or [0, 1, 1]\n",
        "\n",
        "Use .predict() When You:\n",
        "Want to see how the model performs on new or test data\n",
        "\n",
        "Want to generate results for unseen examples\n",
        "\n",
        "Are building a real-world app (like predicting prices, detecting spam, etc.)\n",
        "\n",
        "Common Mistakes to Avoid:\n",
        "Passing labels (y_test) into .predict()\n",
        "\n",
        "Using .predict() before calling .fit()\n",
        "\n",
        "Mismatched feature dimensions between training and test data\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "_sgeUnfRdNCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q20.What are continuous and categorical variables?\n",
        "\n",
        "\"\"\"\"What Are Variables?\n",
        "In data, a variable is any feature or column that can vary between different records or observations. Variables are generally divided into two main types:\n",
        "\n",
        "1. Continuous Variables\n",
        "These are numerical variables that can take on any value within a range—even decimals or fractions.\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 160.5 cm)\n",
        "\n",
        "Weight (e.g., 72.3 kg)\n",
        "\n",
        "Temperature (e.g., 23.8°C)\n",
        "\n",
        "Income (e.g., $54,321.75)\n",
        "\n",
        "You can measure these, and they are often used in regression problems.\n",
        "\n",
        "2. Categorical Variables\n",
        "These are variables that represent categories, groups, or labels. They are often non-numeric, or if numeric, their values represent categories, not quantities.\n",
        "\n",
        "Examples:\n",
        "Gender: Male, Female, Other\n",
        "\n",
        "Color: Red, Blue, Green\n",
        "\n",
        "Education Level: High School, Bachelor's, Master's\n",
        "\n",
        "ZIP Code (numeric but categorical)\n",
        "\n",
        "You can count or group these, and they are usually used in classification problems.\n",
        "\n",
        "Quick Comparison:\n",
        "Feature\tContinuous Variable\tCategorical Variable\n",
        "Type\tNumeric (real numbers)\tCategories or labels\n",
        "Examples\tAge, Salary, Distance\tCountry, Color, Gender\n",
        "Used for\tRegression models\tClassification models\n",
        "Handled by\tScaling (e.g., StandardScaler)\tEncoding (e.g., OneHotEncoder)\n",
        "Machine Learning Implication\n",
        "Continuous variables can go directly into most models (after scaling).\n",
        "\n",
        "Categorical variables must be encoded (like with One-Hot Encoding or Label Encoding) before use.\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "LgyqjCEBdpOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "\"\"\"\"What is Feature Scaling?\n",
        "Feature scaling is the process of normalizing or standardizing the range of features (a.k.a. input variables) so that they're on a similar scale.\n",
        "\n",
        "It's especially important when features have different units or orders of magnitude.\n",
        "\n",
        "Why Is It Important?\n",
        "Machine learning algorithms (especially distance-based or gradient-based ones) work best when features are on a similar scale.\n",
        "\n",
        "Without scaling:\n",
        "\n",
        "Features with large values (e.g., income = 50,000) can dominate smaller ones (e.g., age = 25).\n",
        "\n",
        "The model might give more importance to the wrong features.\n",
        "\n",
        "Helps Improve:\n",
        "ML Algorithm\t                              Needs Scaling?\t                              Why?\n",
        "Linear Regression\t                              Yes\t                                Gradient descent sensitive to scale\n",
        "Logistic Regression                            \tYes\t                                Same reason\n",
        "K-Nearest Neighbors (KNN)\t                      Yes                               \tUses Euclidean distance\n",
        "SVM (Support Vector Machine)\t                  Yes                                \tSensitive to feature magnitude\n",
        "Neural Networks\t                                Yes\t                                Speeds up convergence\n",
        "Tree-based models (Random Forest, XGBoost)\t    Not necessary\t                      Not affected by scale\n",
        "Common Feature Scaling Techniques\n",
        "1. Standardization (Z-score scaling)\n",
        "Transforms data to have:\n",
        "\n",
        "Mean = 0\n",
        "\n",
        "Standard deviation = 1\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "2. Min-Max Scaling (Normalization)\n",
        "Scales all values to a range between 0 and 1 (or any custom range).\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "3. Robust Scaling\n",
        "Useful if your data has outliers. Uses median and interquartile range.\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Example:\n",
        "Feature\tRaw Value\tAfter Min-Max Scaling\n",
        "Age\t20\t0.0\n",
        "Age\t40\t0.5\n",
        "Age\t60\t1.0\n",
        "TL;DR:\n",
        "Feature scaling makes training faster and more stable.\n",
        "\n",
        "It’s a must-do for models like SVM, KNN, and deep learning.\n",
        "\n",
        "Use StandardScaler most of the time, unless you have outliers → then try RobustScaler.\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "0bp9ozG1d7cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q22.How do we perform scaling in Python?\n",
        "\n",
        "\"\"\"\"Step-by-Step: Performing Feature Scaling in Python\n",
        "\n",
        " 1. Import a scaler from sklearn.preprocessing\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler  # or MinMaxScaler, RobustScaler\n",
        "2. Create an instance of the scaler\n",
        "\n",
        "scaler = StandardScaler()   # for standardization\n",
        "Or, for Min-Max scaling:\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()     # scales between 0 and 1\n",
        "3. Fit the scaler on training data and transform\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        ".fit() calculates mean and std (or min/max for MinMaxScaler)\n",
        "\n",
        ".transform() applies scaling\n",
        "\n",
        ".fit_transform() does both at once\n",
        "\n",
        "4. Use the same scaler on test data\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "Always fit on training data only and transform both train and test.\n",
        "This avoids data leakage!\n",
        "\n",
        "Full Example:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset\n",
        "X = [[1, 200], [2, 300], [3, 400], [4, 500]]\n",
        "y = [0, 0, 1, 1]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "Bonus: Scaling with Pipelines (Recommended!)\n",
        "This helps avoid errors and makes your code cleaner:\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\"\"\"\"\""
      ],
      "metadata": {
        "id": "OHCe8ht_ezBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q23.What is sklearn.preprocessing?\n",
        "\n",
        "\"\"\"\"\n",
        "sklearn.preprocessing is a module in Scikit-learn that provides tools to prepare your data before feeding it into a machine learning model.\n",
        "\n",
        "In short: It helps you clean, convert, and scale your data to make it model-ready.\n",
        "\n",
        "What Can You Do with sklearn.preprocessing?\n",
        "You can perform a bunch of important data preprocessing tasks, like:\n",
        "\n",
        "1. Feature Scaling / Normalization\n",
        "Helps bring features to a common scale:\n",
        "\n",
        "StandardScaler – standardizes features (mean = 0, std = 1)\n",
        "\n",
        "MinMaxScaler – scales to a 0–1 range\n",
        "\n",
        "RobustScaler – handles outliers using medians\n",
        "\n",
        "2. Encoding Categorical Variables\n",
        "Turns text categories into numbers:\n",
        "\n",
        "LabelEncoder – encodes labels (for target variables)\n",
        "\n",
        "OneHotEncoder – turns categories into binary vectors\n",
        "\n",
        "OrdinalEncoder – encodes with integer values (for ordered categories)\n",
        "\n",
        "3. Imputing Missing Values\n",
        "Fill in missing data:\n",
        "\n",
        "SimpleImputer – replaces missing values with mean, median, mode, etc.\n",
        "\n",
        "4. Polynomial Features\n",
        "Create new features from combinations of others:\n",
        "\n",
        "PolynomialFeatures – useful for non-linear regression\n",
        "\n",
        "5. Power Transforms\n",
        "Make data more Gaussian-like:\n",
        "\n",
        "PowerTransformer, QuantileTransformer\n",
        "\n",
        "Why It’s Important?\n",
        "Many machine learning models expect numeric and scaled input\n",
        "\n",
        "Raw data often needs cleaning, transforming, and formatting\n",
        "\n",
        "Preprocessing improves model performance and accuracy\n",
        "\n",
        "Example: Scaling + Encoding\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Scaling numerical data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_num)\n",
        "\n",
        "# Encoding categorical data\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(X_cat).toarray()\n",
        "Commonly Used Tools in sklearn.preprocessing\n",
        "Tool\tPurpose\n",
        "StandardScaler\tStandardize features\n",
        "MinMaxScaler\tNormalize to 0–1\n",
        "RobustScaler\tScaling with outlier resistance\n",
        "LabelEncoder\tEncode class labels\n",
        "OneHotEncoder\tEncode categorical features\n",
        "SimpleImputer\tFill in missing values\n",
        "PolynomialFeatures\tAdd polynomial interaction terms\n",
        "PowerTransformer\tNormalize data distribution\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "4Fk6i85mfQTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q24 How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "\"\"\"\"Why Do We Split Data?\n",
        "Training set: Used to teach the model (i.e., model.fit()).\n",
        "\n",
        "Test set: Used to evaluate how well the model performs on unseen data (i.e., model.predict() and metrics like accuracy).\n",
        "\n",
        "How to Split Data in Python (Using train_test_split)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "Syntax:\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "What Each Parameter Means:\n",
        "Parameter\tDescription\n",
        "X\tFeatures (input variables)\n",
        "y\tTarget variable (labels/output)\n",
        "test_size\tProportion of data to keep as the test set (e.g., 0.2 = 20%)\n",
        "random_state\tSets a seed to get reproducible results\n",
        "Example:\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data: 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "Bonus Tips:\n",
        "For classification problems, use stratify=y to keep class distribution consistent:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "You can also split into train, validation, and test sets if needed.\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "h_pA1VcBiyCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q25.Explain data encoding?\n",
        "\n",
        "\"\"\"\"What is Data Encoding?\n",
        "Data encoding is the process of converting categorical (non-numeric) data into numbers so that machine learning models can understand and process them.\n",
        "\n",
        "Most ML algorithms can’t handle strings or labels like \"Red\", \"Blue\", or \"Male\", \"Female\"—they need numbers!\n",
        "\n",
        "Why Is Encoding Important?\n",
        "Categorical data:\n",
        "\n",
        "Can be nominal (no order, like \"red\", \"blue\")\n",
        "\n",
        "Or ordinal (has order, like \"low\", \"medium\", \"high\")\n",
        "\n",
        "You must encode them into numeric values to feed into ML algorithms like logistic regression, decision trees, SVMs, etc.\n",
        "\n",
        "Common Types of Encoding\n",
        "1. Label Encoding\n",
        "Assigns a unique number to each category.\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "color_encoded = le.fit_transform(['Red', 'Green', 'Blue', 'Green'])\n",
        "Example:\n",
        "\n",
        "Color\tEncoded\n",
        "Red\t2\n",
        "Green\t1\n",
        "Blue\t0\n",
        "Simple, but not ideal for nominal data—it introduces order where there is none.\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates a binary column for each category. The value is 1 where the category is present, else 0.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([['Red'], ['Green'], ['Blue']])\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded = encoder.fit_transform(data)\n",
        "Example:\n",
        "\n",
        "Red\tGreen\tBlue\n",
        "1\t0\t0\n",
        "0\t1\t0\n",
        "0\t0\t1\n",
        "Best for nominal data. Avoids giving false numeric priority.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Used for features with a natural order.\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = [['Low'], ['Medium'], ['High']]\n",
        "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "encoded = encoder.fit_transform(data)\n",
        "Example:\n",
        "\n",
        "Level\tEncoded\n",
        "Low\t0\n",
        "Medium\t1\n",
        "High\t2\n",
        "Ideal for ordered categories.\n",
        "\n",
        "Caution:\n",
        "Use One-Hot Encoding for unordered categories.\n",
        "\n",
        "Use Label or Ordinal Encoding for ordered data only.\n",
        "\n",
        "High-cardinality categorical columns (e.g., 1000 unique ZIP codes) may require embedding or target encoding.\n",
        "\n",
        "\"\"\"\""
      ],
      "metadata": {
        "id": "b6Ij4EBSjJvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
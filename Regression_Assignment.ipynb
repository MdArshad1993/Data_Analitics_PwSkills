{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"1. What is Simple Linear Regression?\n",
        "Simple Linear Regression models the linear relationship between one independent variable (X) and one dependent variable (Y) using the equation:\n",
        "Y = mX + c + ε, where:\n",
        "\n",
        "m = slope (rate of change)\n",
        "\n",
        "c = intercept (value of Y when X=0)\n",
        "\n",
        "ε = error term\n",
        "\n",
        "2. Key assumptions:\n",
        "\n",
        "Linearity\n",
        "\n",
        "Independence of errors\n",
        "\n",
        "Homoscedasticity (constant variance)\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "No influential outliers\n",
        "\n",
        "3. What does the coefficient m represent?\n",
        "It indicates how much Y changes for a unit change in X.\n",
        "\n",
        "4. What does the intercept c represent?\n",
        "The value of Y when X = 0; provides a baseline.\n",
        "\n",
        "5. How to calculate slope m?\n",
        "\n",
        "m= ∑(X i− Xˉ)(Y i− Yˉ)/∑(X i− Xˉ) 2\n",
        "​\n",
        "\n",
        "6. Purpose of least squares method:\n",
        "To minimize the sum of squared errors (residuals) and find the best-fitting line.\n",
        "\n",
        "7. R² interpretation:\n",
        "The coefficient of determination (R²) explains how much of the variation in Y is explained by X.\n",
        "\n",
        "R² = 0.8 → 80% of variance in Y is explained by X.\n",
        "\n",
        "📌 Multiple Linear Regression\n",
        "8. What is Multiple Linear Regression?\n",
        "It models the relationship between two or more independent variables and a dependent variable:\n",
        "\n",
        "𝑌=𝛽0+𝛽1𝑋1+𝛽2𝑋2+⋯+𝛽𝑛𝑋𝑛+𝜀\n",
        "\n",
        "9. Main difference from Simple Linear Regression:\n",
        "\n",
        "Simple: one independent variable\n",
        "\n",
        "Multiple: two or more independent variables\n",
        "\n",
        "10. Key assumptions:\n",
        "\n",
        "Linear relationship\n",
        "\n",
        "No or little multicollinearity\n",
        "\n",
        "Independence of errors\n",
        "\n",
        "Homoscedasticity\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "11. Heteroscedasticity and its effect:\n",
        "When error variance is not constant, it can lead to biased standard errors and invalid hypothesis tests.\n",
        "\n",
        "12. Improving multicollinearity:\n",
        "\n",
        "Remove correlated predictors\n",
        "\n",
        "Combine features\n",
        "\n",
        "Use Ridge or Lasso regression\n",
        "\n",
        "Apply PCA (Principal Component Analysis)\n",
        "\n",
        "13. Techniques for categorical variables:\n",
        "\n",
        "One-hot encoding\n",
        "\n",
        "Label encoding\n",
        "\n",
        "Ordinal encoding\n",
        "\n",
        "Binary encoding\n",
        "\n",
        "14. Role of interaction terms:\n",
        "Allows modeling combined effects (e.g., the impact of one variable depends on another).\n",
        "\n",
        "15. Intercept interpretation differences:\n",
        "\n",
        "Simple: value of Y when X = 0\n",
        "\n",
        "Multiple: value of Y when all X variables = 0 (often hypothetical)\n",
        "\n",
        "16. Significance of slope:\n",
        "Shows the rate of change of Y with respect to a predictor. Affects direction and strength of predictions.\n",
        "\n",
        "17. Limitations of R²:\n",
        "\n",
        "Doesn’t penalize complexity\n",
        "\n",
        "Doesn’t imply causation\n",
        "\n",
        "Can’t detect overfitting\n",
        "→ Use Adjusted R², AIC, or cross-validation instead\n",
        "\n",
        "18. Large standard error interpretation:\n",
        "Indicates uncertainty or instability in the coefficient estimate; it may not be statistically significant.\n",
        "\n",
        "19. Identifying heteroscedasticity in residual plots:\n",
        "Look for fanning patterns or cones rather than a random scatter. Important to fix it for valid inference.\n",
        "\n",
        "20. High R² but low adjusted R²:\n",
        "Means too many unnecessary predictors → overfitting.\n",
        "\n",
        "21. Why scale variables in MLR:\n",
        "\n",
        "Standardizes influence of variables\n",
        "\n",
        "Essential for regularized models (e.g., Ridge)\n",
        "\n",
        "Improves interpretability\n",
        "\n",
        "22. What is Polynomial Regression?\n",
        "It fits a non-linear relationship between X and Y using polynomial terms:\n",
        "𝑌=𝛽0+𝛽1𝑋+𝛽2𝑋2+𝛽3𝑋3+⋯+𝜀\n",
        "\n",
        "23. Difference from linear regression:\n",
        "Linear regression fits a straight line; polynomial regression fits a curve.\n",
        "\n",
        "24. When to use polynomial regression:\n",
        "When the data shows non-linear trends not captured by linear models.\n",
        "\n",
        "25. General equation:\n",
        "\n",
        "𝑌=𝛽0+𝛽1𝑋+𝛽2𝑋2+⋯+𝛽𝑛𝑋𝑛+𝜀\n",
        "\n",
        "26. Can it be applied to multiple variables?\n",
        "Yes, using multivariate polynomial regression.\n",
        "\n",
        "27. Limitations of polynomial regression:\n",
        "\n",
        "Sensitive to outliers\n",
        "\n",
        "Risk of overfitting (especially at high degrees)\n",
        "\n",
        "Harder to interpret\n",
        "\n",
        "28. Methods to evaluate polynomial degree:\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "AIC/BIC\n",
        "\n",
        "Adjusted R²\n",
        "\n",
        "Visual inspection of fit\n",
        "\n",
        "29. Why visualization is important:\n",
        "Helps detect underfitting/overfitting, curvature, outliers, and residual patterns.\n",
        "\"\"\"\"\n",
        "#30. How to implement in Python:\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "rAzaaYAzmMbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}